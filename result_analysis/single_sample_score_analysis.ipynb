{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:08:35.838315Z",
     "start_time": "2024-04-08T20:08:35.107328Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import numpy as np\n",
    "from termcolor import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:08:43.189781Z",
     "start_time": "2024-04-08T20:08:42.904784Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # Drop temp_R, R_precision and matching_score_pred columns; not needed with the new print statements\n",
    "    # df = df.drop(columns=['temp_R', 'R_precision', 'matching_score_pred'])\n",
    "\n",
    "    # Remove first 4 characters of token column in dataframe (removes sos/ from token string)\n",
    "    df['token'] = df['token'].str[4:]\n",
    "\n",
    "    # Replace every '/' with ' ' in token column\n",
    "    df['token'] = df['token'].str.replace('/', ' ')\n",
    "\n",
    "    # Remove all words that end with a '_' in token column\n",
    "    df['token'] = df['token'].str.replace(r'\\w*_', '', regex=True).str.strip()\n",
    "\n",
    "    # Remove everything after and including eos (probably stands for end of sentence) in token column\n",
    "    df['token'] = df['token'].str.split('eos').str[0]\n",
    "\n",
    "    return df\n",
    "\n",
    "# --- Load data ---\n",
    "# Load altered.log and original.log, split with ; into dataframes\n",
    "original = pd.read_csv('Z_final_Original_singlescores.log', sep=';')\n",
    "try:\n",
    "    altered = pd.read_csv('Z_final_Evaluation_LeftRight_singlescores_1716565077.01759.log', sep=';')\n",
    "except pd.errors.ParserError as e:\n",
    "    print(e)\n",
    "    print(\"Use VSCode replace function to get rid of unwanten ;/PUNCT characters.\")\n",
    "    exit()\n",
    "# --- Preprocess data ---\n",
    "original = preprocess(original)\n",
    "altered = preprocess(altered)\n",
    "\n",
    "# get to same length\n",
    "max_rows = min(original.shape[0], altered.shape[0])\n",
    "original = original.iloc[:max_rows]\n",
    "altered = altered.iloc[:max_rows]\n",
    "\n",
    "# Add suffix to columns\n",
    "original = original.add_suffix('_original')\n",
    "altered = altered.add_suffix('_altered')\n",
    "\n",
    "# Fuse the two dataframes together\n",
    "fused = pd.concat([original, altered], axis=1)\n",
    "\n",
    "# Remove prefix\n",
    "# Get the length of each token_original\n",
    "fused['token_original_length'] = fused['token_original'].str.len()\n",
    "\n",
    "# Remove the characters from the token_altered column based on the length of token_original\n",
    "#fused['token_altered_no_prefix'] = fused.apply(lambda row: row['token_altered'][row['token_original_length']:], axis=1)\n",
    "\n",
    "fused.drop(columns=['token_original_length'], inplace=True)\n",
    "\n",
    "# Shuffle column order\n",
    "#fused = fused[['temp_match_original', 'temp_match_altered', 'token_original', 'token_altered', 'token_altered_no_prefix']]\n",
    "fused = fused[['temp_match_original', 'temp_match_altered', 'token_original', 'token_altered']]\n",
    "\n",
    "# Save fused dataframe to csv\n",
    "fused.to_csv('fused.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:09:07.942810Z",
     "start_time": "2024-04-08T20:09:07.924811Z"
    }
   },
   "outputs": [],
   "source": [
    "fused[[\"token_original\", \"token_altered\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:09:13.712151Z",
     "start_time": "2024-04-08T20:09:13.668126Z"
    }
   },
   "outputs": [],
   "source": [
    "# --- Analysis ---\n",
    "fused = pd.read_csv('fused.csv')\n",
    "\n",
    "# Replace NaN values with empty string\n",
    "fused = fused.fillna('')\n",
    "\n",
    "print(f\"Mean Org.: {fused['temp_match_original'].mean()}\")\n",
    "print(f\"Mean Alt.: {fused['temp_match_altered'].mean()}\")\n",
    "\n",
    "improvements = fused[fused['temp_match_altered'] < fused['temp_match_original']]\n",
    "degradations = fused[fused['temp_match_altered'] > fused['temp_match_original']]\n",
    "\n",
    "# Figure out how many times the altered temp_match is higher than the original and vice-versa\n",
    "print(f\"Altered better than original: {len(improvements)}\")\n",
    "print(f\"Altered worse than altered: {len(degradations)}\")\n",
    "\n",
    "# Create histogram of temp_match_original and temp_match_altered\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(fused['temp_match_original'], bins=50, alpha=0.5, label='Original')\n",
    "plt.hist(fused['temp_match_altered'], bins=50, alpha=0.5, label='Altered')\n",
    "plt.legend(loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvements['diff'] = improvements['temp_match_original'] - improvements['temp_match_altered']\n",
    "degradations['diff'] = degradations['temp_match_altered'] - degradations['temp_match_original']\n",
    "\n",
    "# Sort by diff\n",
    "improvements = improvements.sort_values(by='diff', ascending=False)\n",
    "degradations = degradations.sort_values(by='diff', ascending=False)\n",
    "\n",
    "cprint('Top 10 improvements', 'green', attrs=['bold'])\n",
    "\"\"\"for i in range(10):\n",
    "    print(colored(\"Original token: \", 'blue') + improvements['token_original'].iloc[i])\n",
    "    try:\n",
    "        print(colored(\"Added info: \", 'blue') + improvements['token_altered_no_prefix'].iloc[i])\n",
    "    except:\n",
    "        print(colored(\"Added info: \", 'blue') + \"-\")\n",
    "    print(colored(\"Improvement: \", 'blue') + f\"{improvements['diff'].iloc[i]}\")\n",
    "    print('\\n')\"\"\"\n",
    "\n",
    "for i in range(15):\n",
    "    print(colored(\"Original token: \", 'blue') + improvements['token_original'].iloc[i])\n",
    "    print(colored(\"Altered token: \", 'blue') + improvements['token_altered'].iloc[i])\n",
    "    print(colored(\"Improvement: \", 'blue') + f\"{improvements['diff'].iloc[i]}\")\n",
    "    print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-08T20:09:15.191337Z",
     "start_time": "2024-04-08T20:09:15.181341Z"
    }
   },
   "outputs": [],
   "source": [
    "cprint('Top 10 degradations', 'red', attrs=['bold'])\n",
    "\"\"\"for i in range(10):\n",
    "    print(colored(\"Original token: \", 'blue') + degradations['token_original'].iloc[i])\n",
    "    try:\n",
    "        print(colored(\"Added info: \", 'blue') + degradations['token_altered_no_prefix'].iloc[i])\n",
    "    except:\n",
    "        print(colored(\"Added info: \", 'blue') + \"-\")\n",
    "    print('\\n')\"\"\"\n",
    "    \n",
    "for i in range(10):\n",
    "    print(colored(\"Original token: \", 'blue') + degradations['token_original'].iloc[i])\n",
    "    print(colored(\"Altered token: \", 'blue') + degradations['token_altered'].iloc[i])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most common words in the token_altered_no_prefix column\n",
    "from collections import Counter\n",
    "\n",
    "words = []\n",
    "for i in range(len(fused)):\n",
    "    words.extend(fused.iloc[i]['token_altered'].split(' '))\n",
    "\n",
    "counter = Counter(words)\n",
    "print(counter.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['leg', 'arm', 'torso', 'knee', 'hip', 'elbow', 'hand', 'forward', 'extend', 'swinge', 'move']\n",
    "\n",
    "for keyword in keywords:\n",
    "    print(f\"Keyword: {keyword}\")\n",
    "    print(f\"Improvements: {improvements['token_altered'].str.contains(keyword).sum()}\")\n",
    "    print(f\"Degradations: {degradations['token_altered'].str.contains(keyword).sum()}\")\n",
    "    total_improvement = improvements[improvements['token_altered'].str.contains(keyword)]['diff'].sum()\n",
    "    total_degradation = degradations[degradations['token_altered'].str.contains(keyword)]['diff'].sum()\n",
    "    print(f\"Total improvement: {total_improvement}\")\n",
    "    print(f\"Total degradation: {total_degradation}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get best and worst quantile (10%) based on temp_match from original for both original and altered prompts\n",
    "original_good = fused[fused['temp_match_original'] < fused['temp_match_original'].quantile(0.1)]\n",
    "original_bad =  fused[fused['temp_match_original'] > fused['temp_match_original'].quantile(0.9)]\n",
    "altered_good = fused[fused['temp_match_altered'] < fused['temp_match_original'].quantile(0.1)]\n",
    "altered_bad =  fused[fused['temp_match_altered'] > fused['temp_match_original'].quantile(0.9)]\n",
    "\n",
    "cols_to_use = original_good.columns.difference(altered_good.columns) # This is essentially just the index\n",
    "\n",
    "# We collect 4 classes:\n",
    "# GG: Original good, Altered good\n",
    "# GB: Original good, Altered bad\n",
    "# BG: Original bad, Altered good\n",
    "# BB: Original bad, Altered bad\n",
    "GG = pd.merge(original_good, altered_good[cols_to_use], left_index=True, right_index=True, how='inner')\n",
    "GB = pd.merge(original_good, altered_bad[cols_to_use], left_index=True, right_index=True, how='inner')\n",
    "BG = pd.merge(original_bad, altered_good[cols_to_use], left_index=True, right_index=True, how='inner')\n",
    "BB = pd.merge(original_bad, altered_bad[cols_to_use], left_index=True, right_index=True, how='inner')\n",
    "\n",
    "GG[['token_original', 'token_altered']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
